{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 8\n",
    "EPSILON = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\", map_name=\"{}x{}\".format(BOARD_SIZE, BOARD_SIZE), is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_dict = {\n",
    "    0: ': LEFT',\n",
    "    1: ': DOWN',\n",
    "    2: ': RIGHT',\n",
    "    3: ': UP',\n",
    "}\n",
    "\n",
    "def print_Q():\n",
    "    last_key = 0\n",
    "    for key, value in Q.items():\n",
    "        if last_key != key[0]:\n",
    "            print()\n",
    "            last_key = key[0]\n",
    "        key_repr = str(key[0]) + translation_dict[key[1]]\n",
    "        print(key_repr, '%7.4f' % value, end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definite_move(state):\n",
    "    state_action_pairs = [(state, action) for action in range(env.action_space.n)]\n",
    "    max_q, best_action = Q[state_action_pairs[0]], state_action_pairs[0][1]\n",
    "    for sa_pair in state_action_pairs:\n",
    "        if Q[sa_pair] > max_q:\n",
    "            max_q = Q[sa_pair]\n",
    "            best_action = sa_pair[1]\n",
    "\n",
    "    return best_action\n",
    "\n",
    "\n",
    "def move_epsilon_greedy(state):\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand < EPSILON:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return definite_move(state)\n",
    "\n",
    "    \n",
    "def move_softmax(state):\n",
    "    state_action_pairs = [(state, action) for action in range(env.action_space.n)]\n",
    "    probabilities = [Q[sa_pair] for sa_pair in state_action_pairs]\n",
    "    normalized_probabilities = [0] * env.action_space.n\n",
    "    \n",
    "    for idx, p in enumerate(probabilities):\n",
    "        p = p / sum(probabilities)\n",
    "        normalized_probabilities[idx] = p + normalized_probabilities[idx-1]\n",
    "    \n",
    "    choice = random.uniform(0, 1)\n",
    "    counter = 0\n",
    "    while choice > normalized_probabilities[counter]:\n",
    "        counter += 1\n",
    "    \n",
    "    return counter\n",
    "\n",
    "\n",
    "def move_by_Q(state, exploration_exploitation_policy='epsilon_greedy'):\n",
    "    if exploration_exploitation_policy == 'epsilon_greedy':\n",
    "        return move_epsilon_greedy(state)\n",
    "    elif exploration_exploitation_policy == 'softmax':\n",
    "        return move_softmax(state)\n",
    "    else:\n",
    "        print(\"What\")\n",
    "\n",
    "    \n",
    "def enhanced_reward(prev_state, new_state):\n",
    "    if new_state == BOARD_SIZE * BOARD_SIZE - 1:\n",
    "        return 50\n",
    "    \n",
    "    if env.desc[new_state // BOARD_SIZE][new_state % BOARD_SIZE] == b'H':\n",
    "        return -10\n",
    "    \n",
    "    row_diff = new_state // BOARD_SIZE - prev_state // BOARD_SIZE\n",
    "    col_diff = new_state % BOARD_SIZE - prev_state % BOARD_SIZE\n",
    "    \n",
    "    if row_diff + col_diff > 0:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "def initialize_Q():\n",
    "    global Q\n",
    "    for state in range(env.observation_space.n):\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[(state, action)] = random.uniform(0.49, 0.51)\n",
    "\n",
    "def learn_Q(DECAY_RATE=0.9, LEARNING_RATE=0.1, MAX_ITER=10000, LR_SCHED=1000, LR_DECAY=0.1):\n",
    "    initialize_Q()\n",
    "    \n",
    "    for i in range(MAX_ITER):\n",
    "        state, reward, done = 0, 0, False\n",
    "        env.reset()        \n",
    "        if i != 0 and i % LR_SCHED == 0:\n",
    "            LEARNING_RATE = LEARNING_RATE * LR_DECAY\n",
    "        while not done:\n",
    "            move = move_by_Q(state)\n",
    "\n",
    "            new_state, _, done, _ = env.step(move)\n",
    "            new_reward = enhanced_reward(state, new_state)\n",
    "            r = new_reward - reward\n",
    "\n",
    "            max_q_new_state = max([Q[(new_state, a)] for a in range(env.action_space.n)])\n",
    "            difference = LEARNING_RATE * (r + DECAY_RATE * max_q_new_state - Q[(state, move)])\n",
    "            Q[(state, move)] = Q[(state, move)] + difference\n",
    "\n",
    "            reward = new_reward\n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_by_Q(silent=True):\n",
    "    env.reset()\n",
    "    \n",
    "    state, done = 0, False\n",
    "    if not silent:\n",
    "        env.render()\n",
    "    while not done:\n",
    "        state, reward, done, _ = env.step(definite_move(state))\n",
    "        if not silent:\n",
    "            env.render()\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def benchmark_Q(runs=10000):\n",
    "    rewards = 0\n",
    "    for i in range(runs):\n",
    "        rewards += run_by_Q()\n",
    "\n",
    "    return rewards / runs * 100\n",
    "\n",
    "def run():\n",
    "    global Q\n",
    "    maximum_win_ratio = 0\n",
    "    best_Q = {}\n",
    "    for i in range(10):\n",
    "        print(\"Learning...\")\n",
    "        learn_Q(MAX_ITER=10000, LR_SCHED=1000, LR_DECAY=0.1)\n",
    "        print(\"Benchmarking policy...\")\n",
    "        win_ratio = benchmark_Q()\n",
    "        print(\"Benchmarking done. Win Ratio: {}\".format(win_ratio))\n",
    "        if win_ratio > maximum_win_ratio:\n",
    "            maximum_win_ratio = win_ratio\n",
    "            best_Q = Q.copy()\n",
    "        print()\n",
    "\n",
    "    Q = best_Q\n",
    "    print(\"Final win ratio: {}\".format(maximum_win_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 39.34\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 31.330000000000002\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 34.82\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 43.730000000000004\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 46.5\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 40.28\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 46.67\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 42.84\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 38.51\n",
      "\n",
      "Learning...\n",
      "Benchmarking policy...\n",
      "Benchmarking done. Win Ratio: 35.589999999999996\n",
      "\n",
      "Final win ratio: 46.67\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
